# ðŸ“˜ Linear Algebra â€” Unit 2: Matrix Transformations

## (Mathematics + Machine Learning Perspective)

---

## 0ï¸âƒ£ Why This Unit Is Non-Negotiable for ML

In ML / DL:

* Every **dense layer** is a matrix transformation
* Every **convolution** is a structured matrix transformation
* Backpropagation = repeated application of **transposes**
* Feature collapse, dead neurons, vanishing gradients = **linear algebra failures**

So this unit answers one question:

> **How does a model reshape information space so learning becomes possible?**

---

## 1ï¸âƒ£ Functions â†’ Transformations â†’ Linear Transformations

---

### ðŸ”¹ Function (Mathematical View)

A function maps elements from one set to another:

```
f: X â†’ Y
```

In linear algebra:

```
f: â„â¿ â†’ â„áµ
```

---

### Example

```
f(x, y) = (2x + y, x âˆ’ y)
```

---

### ðŸ”¹ Transformation (Geometric View)

A **transformation** maps vectors to vectors:

```
T(xâƒ—) = yâƒ—
```

Think:

* Input vector = point in space
* Output vector = new position

---

### ðŸ”¹ Linear Transformation (Formal Definition)

A transformation `T` is **linear** iff:

#### 1. Additivity

```
T(uâƒ— + vâƒ—) = T(uâƒ—) + T(vâƒ—)
```

#### 2. Homogeneity

```
T(cuâƒ—) = cT(uâƒ—)
```

---

### Immediate Consequences

* Origin is fixed: `T(0) = 0`
* Lines remain lines
* Planes remain planes
* No bending or warping â€” only reshaping

---

### ðŸ”¹ ML Interpretation

Linearity means:

* Changes in input cause **proportional changes in output**
* Gradients are stable and predictable
* Optimization is tractable

This is **why neural networks are built from linear layers**.

---

## 2ï¸âƒ£ Linear Transformations as Matrices

---

### ðŸ”¹ Fundamental Theorem

Every linear transformation can be written as:

```
T(x) = A x
```

Where:

* `A` is a matrix
* `x` is a vector

---

### ðŸ”¹ Why This Is True (Key Insight)

A linear transformation is completely defined by where it sends **basis vectors**.

In â„Â²:

```
eâ‚ = (1, 0)
eâ‚‚ = (0, 1)
```

If:

```
T(eâ‚) = aâƒ—
T(eâ‚‚) = bâƒ—
```

Then:

```
A = [ aâƒ—  bâƒ— ]
```

---

### Example

```
T(1, 0) = (2, 1)
T(0, 1) = (âˆ’1, 3)
```

```
A = [  2  âˆ’1
       1   3 ]
```

```
T(x, y) = A Â· [x y]áµ€
```

---

### ðŸ”¹ ML Interpretation

* **Columns of A** â†’ how each input feature contributes
* **Rows of A** â†’ what each neuron extracts

This is **feature recombination**, not just multiplication.

---

## 3ï¸âƒ£ Common Linear Transformations (Math + ML Meaning)

---

### ðŸ”¹ Scaling

```
A = [ k  0
      0  k ]
```

**Math**

* Uniform stretch or shrink
* Area scales by `kÂ²`

**ML**

* Feature amplification or attenuation
* Large values â†’ exploding activations
* Small values â†’ vanishing activations

---

### ðŸ”¹ Non-Uniform Scaling

```
[ a  0
  0  b ]
```

**ML Meaning**

* Unequal importance given to features
* Learned automatically during training

---

### ðŸ”¹ Rotation

```
R(Î¸) = [ cosÎ¸  âˆ’sinÎ¸
         sinÎ¸   cosÎ¸ ]
```

**Math**

* Preserves length and angles
* Determinant = 1

**ML**

* Re-expressing data in a better basis
* PCA and whitening rely on rotations

---

### ðŸ”¹ Shear

```
[ 1  k
  0  1 ]
```

**ML**

* Mixing correlated features
* Creates interactions without changing volume

---

## 4ï¸âƒ£ Matrix Multiplication = Composition of Transformations

---

### ðŸ”¹ Mathematical Meaning

If:

```
Tâ‚(x) = A x
Tâ‚‚(x) = B x
```

Then:

```
Tâ‚‚(Tâ‚(x)) = B A x
```

---

### ðŸ”¹ Order Matters

```
AB â‰  BA
```

This is **geometry**, not algebraic annoyance.

---

### ðŸ”¹ ML Interpretation

Stacking layers:

```
x â†’ Wâ‚x â†’ Wâ‚‚
```

Without activation:

```
Wâ‚‚(Wâ‚x) = (Wâ‚‚Wâ‚)x
```

âž¡ï¸ Depth collapses
âž¡ï¸ No expressive gain

This is why **nonlinearity is essential**.

---

## 5ï¸âƒ£ Determinant â€” The Most Underrated ML Concept

---

### ðŸ”¹ Mathematical Definition (2Ã—2)

```
det([a b; c d]) = ad âˆ’ bc
```

---

### ðŸ”¹ Geometric Meaning

* |det(A)| = area / volume scaling
* Sign = orientation (reflection)
* Zero = collapse to lower dimension

---

### ðŸ”¹ ML Interpretation

| Determinant | ML Meaning            |
| ----------- | --------------------- |
| â‰ˆ 1         | Information preserved |
| > 1         | Feature amplification |
| < 1         | Compression           |
| = 0         | Feature death         |
| < 0         | Sign inversion        |

---

### Example (Rank Collapse)

```
A = [1  1
     2  2]
```

```
det(A) = 0
```

**ML Meaning**

* Rows redundant
* Network loses capacity
* Gradients collapse

---

## 6ï¸âƒ£ Inverse Transformations = Recoverability

---

### ðŸ”¹ Mathematical Definition

```
Aâ»Â¹A = I  â‡”  det(A) â‰  0
```

---

### ðŸ”¹ ML Meaning

> Can we reconstruct the input from this representation?

---

### Where This Matters

* Autoencoders
* Normalizing flows
* Whitening
* Change-of-variables in probability

---

### Example (Information Loss)

```
A = [1  0
     0  0]
```

One dimension erased â†’ unrecoverable.

---

## 7ï¸âƒ£ Transpose â€” The Engine of Backpropagation

---

### ðŸ”¹ Mathematical Property

```
(Ax) Â· y = x Â· (Aáµ€y)
```

---

### ðŸ”¹ Backpropagation Rule

Forward:

```
y = Wx
```

Backward:

```
âˆ‚L/âˆ‚x = Wáµ€ Â· âˆ‚L/âˆ‚y
```

---

### ðŸ”¹ ML Interpretation

* Gradients flow through **transposes**
* Badly conditioned matrices distort gradients
* Orthogonal matrices preserve gradient norms

This explains:

* Vanishing gradients
* Exploding gradients
* Orthogonal initialization

---

## 8ï¸âƒ£ Conditioning, Rank, and Training Stability

---

### ðŸ”¹ Rank

Rank = number of independent directions preserved.

Low rank â†’ feature collapse â†’ poor expressiveness.

---

### ðŸ”¹ Conditioning

```
Îº(W) = Ïƒ_max / Ïƒ_min
```

Large Îº:

* Unstable training
* Sensitive gradients

---

## 9ï¸âƒ£ Bias â€” Breaking Linearity on Purpose

```
y = Wx + b
```

* Not linear (`T(0) â‰  0`)
* Shifts decision boundaries
* Without bias, models are severely limited

---

## ðŸ§  Final Unified Mental Model

* **Matrix** = space reshaper
* **Determinant** = information survival score
* **Rank** = expressive capacity
* **Transpose** = gradient router
* **Inverse** = recoverability
* **Multiplication order** = architecture design

---

## ðŸ”¥ Core Truth

Linear algebra in ML is not abstract math.

It is:

> **Engineering how information moves, compresses, and survives under optimization pressure.**
