
# ğŸ“Œ 1ï¸âƒ£ â€” 30 HOTS Questions (Math + ML)

### **A. Mathematical HOTS (1â€“15)**

(Answer immediately after each question for active learning)

1. **If X is uniform over {1,2,3,4,5}, find E[X].**
   â†’ 3

2. **Find Var(X) for the same distribution.**
   â†’ 2

3. **If P(X=3)=0.5 and sum of all probabilities is 1, and values 1,2,4,5 are equally likely â€” find each.**
   â†’ Remaining prob = 0.5; each = 0.125

4. **For a Normal(Î¼=50, Ïƒ=10) distribution, what % of values lie in [40,60]?**
   â†’ 68% (Empirical Rule: Î¼ Â± 1Ïƒ)

5. **If Z is standard normal, P(Z > 0) = ?**
   â†’ 0.5

6. **If two dice are rolled, distribution of their sum is symmetric or skewed? Why?**
   â†’ Symmetric (triangular PMF, symmetry around 7)

7. **Mean of Poisson(Î»=7)?**
   â†’ 7

8. **Variance of Poisson(Î»=7)?**
   â†’ 7

9. **For a Bernoulli(p) variable, find mean.**
   â†’ p

10. **For Bernoulli(p), find variance.**
    â†’ p(1-p)

11. **Which distribution models waiting time between events?**
    â†’ Exponential

12. **Which distribution models number of events in a time interval?**
    â†’ Poisson

13. **If a dataset has extreme outliers, which central tendency metric is better?**
    â†’ Median

14. **Define CDF in 1 line.**
    â†’ ( F(x) = P(X \le x) )

15. **Relationship between PDF and CDF?**
    â†’ ( f(x) = \frac{d}{dx}F(x) )

---

### **B. Machine Learning HOTS (16â€“30)**

16. **Why do we assume normal distribution in Linear Regression?**
    â†’ Residuals must be normal for valid inference, confidence intervals, p-values

17. **Which distribution fits classification probabilities?**
    â†’ Bernoulli / Categorical / Softmax outputs

18. **Softmax layer outputs what type of distribution?**
    â†’ Categorical probability distribution

19. **Why does Naive Bayes rely on distributions?**
    â†’ It multiplies likelihoods using assumed PDFs

20. **In Bayesian learning, what does the prior represent?**
    â†’ Our belief distribution before seeing data

21. **What distribution does dropout noise in neural networks resemble?**
    â†’ Bernoulli

22. **ReLU outputs follow what kind of distribution trend?**
    â†’ Right-skewed / positively skewed

23. **Which distribution helps detect anomalies?**
    â†’ Normal (values >3Ïƒ flagged)

24. **What does variance in data distribution indicate?**
    â†’ Spread â†’ higher model complexity required

25. **Why is standard scaling effective?**
    â†’ It normalizes data to N(0,1), stabilizing gradients

26. **Which loss function assumes Gaussian errors?**
    â†’ MSE

27. **Which loss function assumes Laplace distribution (heavy tail)?**
    â†’ MAE

28. **Cross entropy loss assumes what distribution?**
    â†’ Probability distribution across classes (Categorical)

29. **In mixture models, each cluster corresponds to what?**
    â†’ A component distribution

30. **GANs: generator tries to match what?**
    â†’ True data distribution

---

# ğŸ“Œ 2ï¸âƒ£ â€” Graphical Cheat Sheet (Mental Visuals)

| Distribution | Shape               | When Used            | Key Params |
| ------------ | ------------------- | -------------------- | ---------- |
| Uniform      | Flat                | Equal chance         | a,b        |
| Normal       | Bell Curve          | Natural data, errors | Î¼,Ïƒ        |
| Bernoulli    | Binary              | Yes/No output        | p          |
| Binomial     | Repeated Bernoulli  | Count of successes   | n,p        |
| Poisson      | Spike at low values | Rare events count    | Î»          |
| Exponential  | Decays downwards    | Time between events  | Î»          |
| Categorical  | Bars                | Classification probs | pâ‚,pâ‚‚,...  |

---

# ğŸ“Œ 3ï¸âƒ£ â€” Python Examples (NumPy, Scikit)

```python
import numpy as np
from scipy.stats import norm, poisson, uniform

# Uniform distribution (1-5)
data = np.array([1,2,3,4,5])
print("Mean:", np.mean(data))
print("Std Dev:", np.std(data))

# Generate from Normal
normal_samples = norm.rvs(loc=0, scale=1, size=1000)

# Poisson sample
poisson_samples = poisson.rvs(mu=4, size=1000)

# Prob of value 7 in Poisson(4)
print(poisson.pmf(7, 4))
```

---

# ğŸ“Œ 4ï¸âƒ£ â€” Real-World ML Case Studies

| Application                         | Distribution            | Why                     |
| ----------------------------------- | ----------------------- | ----------------------- |
| Spam vs Ham classification          | Bernoulli & Multinomial | Word presence/absence   |
| Predicting defects in manufacturing | Poisson                 | Rare event counts       |
| Height/Weight data preprocessing    | Normal                  | Natural biological data |
| Customer waiting time prediction    | Exponential             | Time until next arrival |
| Image pixel intensity modeling      | Gaussian mixture        | Complex multimodal data |

---

# ğŸ“Œ 5ï¸âƒ£ â€” Deep Learning Interpretation

### ğŸ”¥ Key Takeaways

* Neural network weights initialize from **Normal** or **Xavier Uniform** distributions â†’ avoids exploding gradients
* Dropout â†’ **Bernoulli**, controlling co-adaptation
* Cross-entropy â†’ assumes **Categorical** predicted distribution
* Generative models â†’ learn entire **data distribution**

### GANs Equation (Distribution Matching)

[
G(z) \rightarrow p_g(x) \approx p_{data}(x)
]

Discriminator objective:

[
\max_D ; \mathbb{E}*{x\sim p*{data}}\log D(x)

* \mathbb{E}_{z\sim p_z}\log (1 - D(G(z)))
  ]

---

# ğŸ¯ Final Summary (Your Brain Should Store This)

| Concept            | In 5 Secs                                     |
| ------------------ | --------------------------------------------- |
| Mean               | Center                                        |
| Variance           | Spread                                        |
| Std Dev            | Square root of variance                       |
| Distribution       | Pattern of data                               |
| ML Goal            | Approximate + use distributions to generalize |
| Deep Learning Goal | Learn data distribution itself                |


---
# Questions in Detail
# ğŸ“Œ PART A â€” MATHEMATICS (1â€“15)

### Mean / Variance / Probability / Distributions

---

### **1ï¸âƒ£ If X is uniform over {1,2,3,4,5}, find E[X]**

A **uniform distribution** means each value has equal probability.
Mean formula:

[
E[X] = \frac{1+2+3+4+5}{5}=3
]

ğŸ“**Meaning:**
If you randomly pick from 1â€“5 thousands of times, the **average** result will be **around 3**.

---

### **2ï¸âƒ£ Find Var(X) for the same distribution**

Variance measures how *spread out* values are from the mean.

[
\mu = 3
]

[
Var(X)=\frac{(1-3)^2+(2-3)^2+(3-3)^2+(4-3)^2+(5-3)^2}{5}
]

[
= \frac{4+1+0+1+4}{5}=2
]

ğŸ“**Interpretation:**
Data is not tightly clustered, but spread evenly around the mean.

---

### **3ï¸âƒ£ P(X=3)=0.5; others equal. Find probability of each.**

Total probability must equal 1:

[
P(1)+P(2)+P(4)+P(5) = 0.5
]

There are 4 equal-probability values, so:

[
P(\text{each}) = 0.5/4 = 0.125
]

ğŸ“**Intuition:**
Model is **biased toward 3** â€” the dataset has many 3s.

---

### **4ï¸âƒ£ Normal(Î¼=50, Ïƒ=10). What % lie in [40,60]?**

Apply **68â€“95â€“99.7 Rule** (Empirical Rule):

* Î¼ Â± 1Ïƒ â†’ 68% of data

So,

[
50 - 10 = 40, \quad 50 + 10 = 60
]

â¡ï¸ **68% of values fall within 40â€“60**

ğŸ“**Why ML cares:**
Normality assumption is required for statistical inference in regression.

---

### **5ï¸âƒ£ If Z is standard normal, P(Z > 0) = ?**

Standard normal is symmetric around 0:

[
P(Z=0) = 0.5
]

ğŸ“**Interpretation:**
Half values are above average; half below.

---

### **6ï¸âƒ£ Two dice rolled â€” is the sum symmetric?**

Possible sums: 2 to 12
Distribution peaks at **7**.

| Sum | # Combos |
| --- | -------- |
| 2   | 1        |
| 3   | 2        |
| 4   | 3        |
| 5   | 4        |
| 6   | 5        |
| 7   | 6        |
| 8   | 5        |
| 9   | 4        |
| 10  | 3        |
| 11  | 2        |
| 12  | 1        |

Symmetric triangle â†’ **Yes**.

ğŸ“**Why important:**
ML often checks **symmetry / skewness** to pick models (e.g. normal vs skewed).

---

### **7ï¸âƒ£ Mean of Poisson(Î»=7)**

[
E[X] = Î» = 7
]

ğŸ“**Used for:**
Count of events (calls per hour, customer arrivals, machine faults).

---

### **8ï¸âƒ£ Variance of Poisson(Î»=7)**

[
Var(X) = Î» = 7
]

ğŸ“**Interpretation:**
If average events are 7, expect variation â‰ˆ7.

---

### **9ï¸âƒ£ Mean of Bernoulli(p)**

Bernoulli: outcomes **0 or 1**

[
E[X] = p
]

ğŸ“If p = 0.8 â†’ 80% chance success.

---

### **ğŸ”Ÿ Variance of Bernoulli(p)**

[
Var(X) = p(1-p)
]

Max variance when p = 0.5
ğŸ“Meaning: Most uncertainty when outcome is **unpredictable**.

---

### **1ï¸âƒ£1ï¸âƒ£ Distribution for waiting time between events? â€” Exponential**

Example:
Time until next bus arrives.

ğŸ“Relation:

If events per hour follow Poisson â†’ waiting times follow **Exponential**.

---

### **1ï¸âƒ£2ï¸âƒ£ Distribution for # of events per interval? â€” Poisson**

Example:
Number of errors on a machine per day.

ğŸ“Poisson & Exponential are siblings.

---

### **1ï¸âƒ£3ï¸âƒ£ Best average when data has outliers?**

âœ”ï¸ **Median**

Example dataset:
1, 2, 3, 4, 1000

Mean = 202
Median = 3

ğŸ“Median resists outliers â†’ **robust**.

---

### **1ï¸âƒ£4ï¸âƒ£ Define CDF**

[
F(x)=P(X \le x)
]

Shows **cumulative probability up to x**.

ğŸ“Used for probability comparisons.

---

### **1ï¸âƒ£5ï¸âƒ£ Relationship between PDF and CDF**

For continuous variables:

[
f(x)=\frac{d}{dx}F(x)
]

PDF is the **rate of change** of the CDF.

ğŸ“In ML â†’ determines likelihoods used in models.

---

# ğŸ“Œ PART B â€” MACHINE LEARNING (16â€“30)

---

### **1ï¸âƒ£6ï¸âƒ£ Why assume normal residuals in Linear Regression?**

Residuals = errors between predictions & true values.

Normal residuals â†’ good properties:

* unbiased coefficients
* valid hypothesis tests
* reliable confidence intervals

ğŸ“If residuals are NOT normal â†’ model is unreliable.

---

### **1ï¸âƒ£7ï¸âƒ£ Classification outputs match which distribution?**

* **Binary â†’ Bernoulli**
* **Multi-class â†’ Categorical**

Example:
Dog vs Cat â†’ Bernoulli(0.8)

---

### **1ï¸âƒ£8ï¸âƒ£ Softmax outputs what?**

A **probability distribution** where:

[
\sum p_i = 1
]

Example:
Softmax([2,4,6]) â†’ [0.01, 0.09, 0.90]

ğŸ“Model believes class 3 with 90% confidence.

---

### **1ï¸âƒ£9ï¸âƒ£ Why Naive Bayes needs distributions?**

Computes:

[
P(Y|X) \propto P(X|Y)P(Y)
]

It uses **probability of features given class**.
Assumes independence â†’ Naive.

ğŸ“Simplifies computation â†’ fast classification.

---

### **2ï¸âƒ£0ï¸âƒ£ Bayesian Learning â€” What is a Prior?**

Before seeing data:

[
P(\theta) = \text{prior belief}
]

Example:
A new doctor assumes patients usually recover â€” **prior belief**.

---

### **2ï¸âƒ£1ï¸âƒ£ Dropout Noise Distribution â†’ Bernoulli**

Neurons **randomly removed** during training:

* keep(1) with p
* drop(0) with 1-p

ğŸ“Prevents overfitting.

---

### **2ï¸âƒ£2ï¸âƒ£ ReLU output distribution â€” Right skewed**

Because:

[
ReLU(x)=\max(0,x)
]

Most outputs become **0s**, rest positive.

ğŸ“Gradient issues â†’ solved by Leaky ReLU / GELU.

---

### **2ï¸âƒ£3ï¸âƒ£ Which distribution helps detect anomalies?**

**Normal distribution** thresholds:

[
|x - \mu| > 3\sigma
]

â†’ Outlier / anomaly

---

### **2ï¸âƒ£4ï¸âƒ£ Variance in data â†’ ML meaning**

High variance â†’ data is **spread** â†’ complex model needed.
Low variance â†’ **simpler** model may be enough.

ğŸ“Ties to **Biasâ€“Variance Tradeoff**.

---

### **2ï¸âƒ£5ï¸âƒ£ Why Standard Scaling works?**

Transforms to:

[
z = \frac{x - \mu}{\sigma}
]

â†’ **N(0,1)**

ğŸ“Neural networks & kNN behave better.

---

### **2ï¸âƒ£6ï¸âƒ£ MSE assumes Gaussian noise**

Why?

[
(y - \hat{y})^2
]

Comes from maximizing likelihood of normal distribution.

ğŸ“Suitable when **errors are normally distributed**.

---

### **2ï¸âƒ£7ï¸âƒ£ MAE assumes Laplace distribution**

[
|y - \hat{y}|
]

Better when data has **outliers**, heavy-tailed distribution.

---

### **2ï¸âƒ£8ï¸âƒ£ Cross Entropy assumes Categorical**

Penalty for wrong probability:

[
-\log(P(\text{correct class}))
]

ğŸ“Used in neural networks.

---

### **2ï¸âƒ£9ï¸âƒ£ In mixture models, each cluster is a distribution**

Example: GMM

[
p(x)=\sum_{k} \pi_k \mathcal{N}(x|\mu_k,\sigma_k)
]

Each cluster â†’ its own Normal.

---

### **3ï¸âƒ£0ï¸âƒ£ GANs â€” What does generator learn?**

Goal:

[
p_g(x) \approx p_{data}(x)
]

Generator tries to create data indistinguishable from real.
Discriminator tries to catch it.

ğŸ“They **compete â†’ improve**.

---

# ğŸ¯ FINAL SUMMARY TABLE

| Topic        | Core Idea                              |
| ------------ | -------------------------------------- |
| Mean         | Expected value                         |
| Variance     | Spread of data                         |
| Std Dev      | Natural scale of deviation             |
| Distribution | Shape of data + probabilities          |
| In ML        | Drives model choice, assumptions, loss |
| In DL        | Target is to learn data distribution   |
