# Mean, Median, Variance & Standard Deviation

## (With Examples + ML Intuition)

---

## 1ï¸âƒ£ MEAN â€” Examples + ML View

### Example 1: Simple data

[
x = [3,;5,;7]
]

[
\mu = \frac{3+5+7}{3} = 5
]

Interpretation:

* The data balances at **5**
* Deviations: (-2, 0, +2)

---

### Example 2: Outlier effect

[
x = [3,;5,;7,;100]
]

[
\mu = \frac{115}{4} = 28.75
]

**Mean shifts heavily** due to one extreme value.

---

### ğŸ¤– ML Intuition (Mean)

In ML, the mean is:

* **Best constant prediction** under squared error loss
* If you predict a single value (c) for all points, the value that minimizes:
  [
  \sum (x_i - c)^2
  ]
  is:
  [
  \boxed{c = \mu}
  ]

â¡ï¸ Mean = **least-squares optimal prediction**

---

## 2ï¸âƒ£ MEDIAN â€” Examples + ML View

### Example 1

[
[1,;2,;3,;4,;5]
]

Median = **3**

---

### Example 2 (outlier resistant)

[
[1,;2,;3,;4,;1000]
]

Median = **3**

Mean = **202**

---

### ğŸ¤– ML Intuition (Median)

Median minimizes:
[
\sum |x_i - c|
]

â¡ï¸ Median = **best prediction under absolute error loss (L1)**

This is why:

* MAE â†’ median
* MSE â†’ mean

Very important ML distinction.

---

## 3ï¸âƒ£ VARIANCE â€” Worked Examples + ML View

### Example 1

[
x = [2,;4,;6]
]

Mean:
[
\mu = 4
]

Deviations:
[
[-2,;0,;2]
]

Squared deviations:
[
[4,;0,;4]
]

Variance:
[
\sigma^2 = \frac{4+0+4}{3} = \frac{8}{3}
]

---

### Example 2: Compare two datasets

A:
[
[4,;4,;4]
]
Variance = **0**

B:
[
[1,;4,;7]
]
Variance = **6**

Same mean, very different spread.

---

### ğŸ¤– ML Intuition (Variance)

Variance answers:

> How uncertain is the data?

In ML:

* High variance â†’ noisy data
* Low variance â†’ stable data

Used in:

* Biasâ€“Variance tradeoff
* Uncertainty estimation
* Model evaluation

---

## 4ï¸âƒ£ STANDARD DEVIATION â€” Examples + ML View

### From previous example

[
\sigma^2 = \frac{8}{3}
]

[
\sigma = \sqrt{\frac{8}{3}} \approx 1.63
]

Meaning:

* Typical distance from mean â‰ˆ **1.63 units**

---

### Example 2: Exam-style

If:
[
\mu = 50,;\sigma = 10
]

Then:

* Most values lie between **40 and 60**
* Very few below 20 or above 80

---

### ğŸ¤– ML Intuition (Standard Deviation)

Standard deviation is:

* Scale of noise
* Width of distribution

In **Gaussian (Normal) distribution**:
[
\mathcal{N}(\mu,\sigma^2)
]

Larger Ïƒ â†’ flatter curve
Smaller Ïƒ â†’ sharper peak

---

## 5ï¸âƒ£ BIG ML CONNECTION (EXTREMELY IMPORTANT)

### Mean Squared Error (MSE)

[
\text{MSE} = \frac{1}{n}\sum (y_i - \hat{y}_i)^2
]

This is literally:

* Variance when prediction = mean
* Squared distance from predictions

### Linear Regression

* Finds parameters that minimize **variance of errors**
* Equivalent to minimizing squared deviations

---

## 6ï¸âƒ£ Biasâ€“Variance Insight (Exam + ML)

| Situation     | Meaning              |
| ------------- | -------------------- |
| High bias     | Model too simple     |
| High variance | Model too sensitive  |
| Low variance  | Stable predictions   |
| Low bias      | Accurate predictions |

Standard deviation measures **variance of errors**.

---

## 7ï¸âƒ£ Visual Intuition (Think This Way)

Imagine dots on a line:

```
|---x---x---Î¼---x---x---|
```

* Mean â†’ center
* Std dev â†’ width of cloud
* Variance â†’ squared width

---

## 8ï¸âƒ£ QUICK COMPARISON (FINAL)

| Measure  | Math Meaning         | ML Meaning              |
| -------- | -------------------- | ----------------------- |
| Mean     | Balance point        | Least-squares predictor |
| Median   | Middle value         | L1-optimal predictor    |
| Variance | Avg squared distance | Error magnitude         |
| Std Dev  | Avg distance         | Noise scale             |

---

## 9ï¸âƒ£ EXAM TRICKS YOU MUST KNOW

âœ” Mean minimizes squared error
âœ” Median minimizes absolute error
âœ” Std dev has same units as data
âœ” Variance is always non-negative
âœ” Zero variance â†’ all values equal


---

# ğŸ”¥ Biasâ€“Variance Tradeoff (DETAILED & INTUITIVE)

---

## 1ï¸âƒ£ What Are We Actually Trying to Do?

In supervised ML, we want a function (\hat{f}(x)) that approximates the true function (f(x)).

Observed data:
[
y = f(x) + \varepsilon
]
where:

* (f(x)) = true relationship
* (\varepsilon) = noise (irreducible error)

---

## 2ï¸âƒ£ ERROR DECOMPOSITION (CORE MATH)

Expected prediction error:

[
\mathbb{E}[(y - \hat{f}(x))^2]
]

This decomposes into:

[
\boxed{
\text{Total Error}
==================

\text{Bias}^2
+
\text{Variance}
+
\text{Noise}
}
]

---

## 3ï¸âƒ£ BIAS â€” WHAT IT REALLY MEANS

### Definition

[
\text{Bias} = \mathbb{E}[\hat{f}(x)] - f(x)
]

### Interpretation

> How far **average prediction** is from the true function.

### High Bias Model

* Too simple
* Strong assumptions
* Cannot capture patterns

### Examples

* Linear regression on nonlinear data
* Shallow decision tree

### Result

â¡ï¸ **Underfitting**

---

## 4ï¸âƒ£ VARIANCE â€” WHAT IT REALLY MEANS

### Definition

[
\text{Variance} = \mathbb{E}\left[(\hat{f}(x) - \mathbb{E}[\hat{f}(x)])^2\right]
]

### Interpretation

> How much predictions change if training data changes.

### High Variance Model

* Too complex
* Memorizes noise
* Sensitive to small dataset changes

### Examples

* Deep decision tree
* High-degree polynomial

### Result

â¡ï¸ **Overfitting**

---

## 5ï¸âƒ£ NOISE (IRREDUCIBLE ERROR)

* Comes from randomness in data
* Measurement error
* Missing variables

â— You **cannot reduce noise** by changing the model.

---

## 6ï¸âƒ£ VISUAL INTUITION (TARGET ANALOGY ğŸ¯)

Imagine darts thrown at a target:

| Situation                | Meaning            |
| ------------------------ | ------------------ |
| High bias, low variance  | Consistently wrong |
| Low bias, high variance  | Random predictions |
| Low bias, low variance   | Ideal model        |
| High bias, high variance | Worst case         |

---

## 7ï¸âƒ£ MATHEMATICAL EXAMPLE (VERY IMPORTANT)

True function:
[
f(x) = x^2
]

### Model 1: Linear regression

[
\hat{f}(x) = ax + b
]

* High bias
* Low variance

### Model 2: 10-degree polynomial

* Low bias
* High variance

### Optimal model:

* Moderate complexity
* Balanced bias & variance

---

## 8ï¸âƒ£ RELATION TO MEAN & VARIANCE (KEY LINK ğŸ”‘)

Recall:

* Mean minimizes squared error
* Variance measures spread of predictions

### High variance model:

* Predictions have large standard deviation

### Bias:

* Mean prediction is shifted away from truth

---

## 9ï¸âƒ£ TRAINING vs TEST ERROR (CRUCIAL)

| Model    | Train Error | Test Error |
| -------- | ----------- | ---------- |
| Underfit | High        | High       |
| Overfit  | Low         | High       |
| Good fit | Low         | Low        |

This gap **is variance**.

---

## ğŸ”Ÿ HOW TO CONTROL BIAS & VARIANCE (PRACTICAL)

### To Reduce Bias:

âœ” Increase model complexity
âœ” Add features
âœ” Reduce regularization

### To Reduce Variance:

âœ” More data
âœ” Regularization (L1/L2)
âœ” Feature selection
âœ” Early stopping
âœ” Bagging / Random Forests

---

## 1ï¸âƒ£1ï¸âƒ£ REGULARIZATION CONNECTION

### Ridge Regression (L2)

[
\text{Loss} = \text{MSE} + \lambda \sum w^2
]

* Penalizes large weights
* Reduces variance
* Slightly increases bias

### Lasso (L1)

[
\text{Loss} = \text{MSE} + \lambda \sum |w|
]

* Feature selection
* Bias â†‘, variance â†“

---

## 1ï¸âƒ£2ï¸âƒ£ BIASâ€“VARIANCE CURVE (EXAM FAVORITE)

As model complexity increases:

* Bias â†“
* Variance â†‘
* Test error â†’ U-shaped curve

Minimum point = **best tradeoff**

---

## 1ï¸âƒ£3ï¸âƒ£ STANDARD DEVIATION VIEW (VERY INTUITIVE)

If you train the same model on different samples:

* High variance â†’ predictions scatter widely
* Std dev of predictions is large

---

## 1ï¸âƒ£4ï¸âƒ£ ML INTERVIEW / EXAM ONE-LINERS

âœ” Bias = error due to wrong assumptions
âœ” Variance = error due to sensitivity
âœ” Overfitting = low bias, high variance
âœ” Underfitting = high bias, low variance
âœ” Regularization trades bias for variance

---

## 1ï¸âƒ£5ï¸âƒ£ HOW TO MASTER THIS (STUDY PLAN)

1. Draw biasâ€“variance curves
2. Practice identifying underfit vs overfit
3. Connect loss functions to variance
4. Relate std dev of errors to variance

---

## ğŸ”¥ FINAL TAKEAWAY

> **Good models are not the most complex or simplest â€” they are balanced.**

Understanding biasâ€“variance tradeoff means you now know:

* *Why models fail*
* *How to fix them*
* *What to change when accuracy drops*

---

# 1ï¸âƒ£ Draw Biasâ€“Variance Curves (and Understand Them)

### Axes

* **X-axis:** Model complexity
  (simple â†’ complex)
* **Y-axis:** Error

---

### Conceptual Curves

```
Error
 ^
 |\
 | \        Test Error
 |  \      /
 |   \    /
 |    \__/      â† U-shaped curve
 |     /
 |    /
 |   /
 |  /
 | /  Bias
 |/________________> Model Complexity
       Variance â†‘
```

---

### What Each Curve Means

#### Bias curve

* High when model is **too simple**
* Decreases as complexity increases

```
Bias Error
 |
 |\
 | \
 |  \
 |   \
 |____\__________
```

#### Variance curve

* Low for simple models
* Explodes for complex models

```
Variance Error
 |
 |     /
 |    /
 |   /
 |  /
 | /
 |/___________
```

---

### ğŸ”‘ Exam Insight

âœ” Minimum **test error** occurs where **biasÂ² + variance** is minimum
âœ” Training error **always decreases** with complexity
âœ” Test error is **U-shaped**

---

# 2ï¸âƒ£ Identify Underfitting vs Overfitting (PRACTICAL SKILL)

This is asked *constantly* in exams and interviews.

---

## Underfitting (High Bias, Low Variance)

### Characteristics

* Model too simple
* Misses patterns
* Poor performance everywhere

### Error Pattern

| Dataset  | Error |
| -------- | ----- |
| Training | High  |
| Test     | High  |

### Examples

* Linear model for curved data
* Very shallow decision tree

### Fix

âœ” Increase complexity
âœ” Add features
âœ” Reduce regularization

---

## Overfitting (Low Bias, High Variance)

### Characteristics

* Model too complex
* Fits noise
* Poor generalization

### Error Pattern

| Dataset  | Error    |
| -------- | -------- |
| Training | Very low |
| Test     | High     |

### Examples

* Deep decision tree
* High-degree polynomial

### Fix

âœ” More data
âœ” Regularization
âœ” Pruning / early stopping

---

## ğŸ”¥ Golden Rule

> **If training error â‰ª test error â†’ variance problem**
> **If both errors are high â†’ bias problem**

---

# 3ï¸âƒ£ Connect Loss Functions to Variance (CRUCIAL LINK)

This is where statistics meets ML optimization.

---

## Mean Squared Error (MSE)

[
\text{MSE} = \frac{1}{n}\sum (y_i - \hat{y}_i)^2
]

### What it does

* Penalizes large errors heavily
* Encourages **low variance predictions**

### Why?

Squaring deviations = exactly how variance is defined.

â¡ï¸ Minimizing MSE â‰ˆ minimizing **variance of errors**

---

## Mean Absolute Error (MAE)

[
\text{MAE} = \frac{1}{n}\sum |y_i - \hat{y}_i|
]

### What it does

* Linear penalty
* Robust to outliers
* Allows higher variance

---

## ML Interpretation Table

| Loss | Encourages         | Bias | Variance |
| ---- | ------------------ | ---- | -------- |
| MSE  | Stable predictions | â†“    | â†“        |
| MAE  | Robust predictions | â†‘    | â†‘        |

---

### ğŸ”‘ Exam Statement

âœ” Squared loss = variance-sensitive
âœ” Absolute loss = outlier-resistant

---

# 4ï¸âƒ£ Relate Standard Deviation of Errors to Variance

This is the **most intuitive connection**.

---

## Errors (Residuals)

[
e_i = y_i - \hat{y}_i
]

### Variance of errors

[
\text{Var}(e) = \frac{1}{n}\sum (e_i - \bar{e})^2
]

If mean error â‰ˆ 0 (good model):

[
\text{Var}(e) \approx \frac{1}{n}\sum e_i^2 = \text{MSE}
]

ğŸ”¥ **This is a BIG result**

---

## Standard Deviation of Errors

[
\sigma_e = \sqrt{\text{Var}(e)}
]

### Interpretation

* Typical prediction error
* Measure of model **stability**

---

## High Variance Model

```
Errors:
|--*----*---*-----*---|
         mean
```

* Errors spread out
* Large std dev
* Unstable predictions

---

## Low Variance Model

```
Errors:
|----*--*--*--*-----|
       mean
```

* Errors tightly clustered
* Small std dev
* Stable predictions

---

# ğŸ”— EVERYTHING CONNECTED (MASTER VIEW)

| Concept           | Meaning               |
| ----------------- | --------------------- |
| Bias              | Mean error            |
| Variance          | Spread of predictions |
| Std dev of errors | Typical mistake size  |
| MSE               | Variance of errors    |
| Overfitting       | High variance         |
| Underfitting      | High bias             |

---

# ğŸ¯ FINAL EXAM-READY SUMMARY

âœ” Biasâ€“variance curve explains optimal complexity
âœ” Underfit â†’ high bias, overfit â†’ high variance
âœ” MSE directly controls variance
âœ” Std dev of errors measures prediction stability
